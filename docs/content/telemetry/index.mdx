import { CodeBlock } from '@/components/CodeBlock'

# Telemetry System

Rhesis includes a privacy-focused telemetry system to collect and analyze usage patterns from both cloud-hosted and self-hosted instances. This helps us understand how the platform is used and improve it for everyone.

<Callout type="info">
  **Privacy First**: All telemetry is optional, privacy-focused, and
  transparent. User and organization IDs are hashed using SHA-256, and sensitive
  data like passwords and API keys are automatically filtered out.
</Callout>

## Architecture Overview

The telemetry system consists of three main components working together:

<CodeBlock filename="example.py" language="python">
{`┌────────────────────────────────────────────────────────┐
│  Rhesis Instances (Cloud + Self-Hosted)                │
│  ┌──────────┐              ┌──────────┐                │
│  │ Backend  │              │ Frontend │                │
│  └────┬─────┘              └────┬─────┘                │
│       │ OTLP (gRPC/HTTP)        │                      │
└───────┼─────────────────────────┼──────────────────────┘
        │                         │
        └──────────┬──────────────┘
                   ▼
        ┌──────────────────────┐
        │ OpenTelemetry        │  Port 4317 (gRPC)
        │ Collector            │  Port 4318 (HTTP)
        └──────────┬───────────┘
                   │ OTLP gRPC
                   ▼
        ┌──────────────────────┐
        │ Telemetry            │  Port 4317 (gRPC)
        │ Processor            │  Process & Store
        └──────────┬───────────┘
                   │ SQL
                   ▼
        ┌──────────────────────┐
        │ PostgreSQL           │  Separate Analytics DB
        │ Analytics Database   │  (Isolated from main DB)
        └──────────────────────┘
`}
</CodeBlock>

### Components

1. **OpenTelemetry Collector** - Receives telemetry from user instances, filters sensitive data, and forwards to the processor
2. **Telemetry Processor** - gRPC service that processes traces and stores structured analytics data
3. **Analytics Database** - Separate PostgreSQL database for analytics, isolated from operational data

## What Data We Collect

<Callout type="default">
  The telemetry system collects **usage patterns** to help us understand how
  Rhesis is used and identify areas for improvement.
</Callout>

### User Activity

- Login/logout events
- Session duration
- Deployment type (cloud or self-hosted)
- Hashed user and organization IDs (SHA-256, irreversible)

### Endpoint Usage

- API endpoint paths and HTTP methods
- Response status codes
- Request duration (performance metrics)
- Timestamp of requests

### Feature Usage

- Feature interactions (created, viewed, updated, deleted)
- Feature names (e.g., "test-run", "test-set", "endpoint")
- Usage timestamps
- Deployment context

## What We DON'T Collect

<Callout type="warning">
  **Privacy Protection**: The following data is automatically filtered and NEVER
  stored:
</Callout>

- ❌ Passwords or password hashes
- ❌ API keys or tokens
- ❌ Authentication credentials
- ❌ Personal Identifiable Information (PII)
- ❌ Test content or user-generated data
- ❌ Email addresses or usernames
- ❌ IP addresses or device identifiers
- ❌ Any sensitive business data

### ID Hashing

All user and organization IDs are one-way hashed before storage:

<CodeBlock filename="example.py" language="python">
{`# Example: SHA-256 hash truncated to 16 characters
hash = hashlib.sha256(id_str.encode()).hexdigest()[:16]
# Original ID: "user-123-456-789"
# Stored hash: "a1b2c3d4e5f6g7h8" (cannot be reversed)
`}
</CodeBlock>

**Properties:**

- ✅ **One-way**: Cannot recover original IDs
- ✅ **Consistent**: Same ID always produces the same hash for analytics
- ✅ **Anonymous**: No PII stored
- ✅ **Collision-resistant**: 2^64 unique values

## Privacy & Security

### Opt-In/Opt-Out

- Telemetry respects user preferences
- Can be disabled entirely for self-hosted instances
- No data collection without explicit configuration

### Data Isolation

- Analytics database is **completely separate** from operational data
- Different access controls and backup policies
- Can be managed independently
- No impact on application performance

### Security Measures

- Sensitive attributes filtered at the collector level
- Batch processing with retry logic for reliability
- Memory limits to prevent resource exhaustion
- Health checks and monitoring built-in

## Ports & Endpoints

### OpenTelemetry Collector

- **4317**: OTLP gRPC receiver (primary)
- **4318**: OTLP HTTP receiver (web apps)
- **8888**: Collector metrics (Prometheus)
- **13133**: Health check endpoint
- **55679**: Debug zpages

### Telemetry Processor

- **4317**: gRPC server for receiving traces from collector

## Database Schema

The analytics database uses three tables with consistent structure:

### `user_activity`

Tracks user engagement events

| Column            | Type         | Description                |
| ----------------- | ------------ | -------------------------- |
| `id`              | UUID         | Primary key                |
| `user_id`         | VARCHAR(32)  | Hashed user ID             |
| `organization_id` | VARCHAR(32)  | Hashed org ID              |
| `event_type`      | VARCHAR(50)  | Event type (login, logout) |
| `timestamp`       | TIMESTAMP    | Event time                 |
| `session_id`      | VARCHAR(255) | Session identifier         |
| `deployment_type` | VARCHAR(50)  | cloud / self-hosted        |
| `event_metadata`  | JSONB        | Additional context         |

### `endpoint_usage`

Tracks API usage and performance

| Column            | Type             | Description         |
| ----------------- | ---------------- | ------------------- |
| `id`              | UUID             | Primary key         |
| `endpoint`        | VARCHAR(255)     | API endpoint path   |
| `method`          | VARCHAR(10)      | HTTP method         |
| `user_id`         | VARCHAR(32)      | Hashed user ID      |
| `organization_id` | VARCHAR(32)      | Hashed org ID       |
| `status_code`     | INTEGER          | HTTP status         |
| `duration_ms`     | DOUBLE PRECISION | Request duration    |
| `timestamp`       | TIMESTAMP        | Request time        |
| `deployment_type` | VARCHAR(50)      | cloud / self-hosted |
| `event_metadata`  | JSONB            | Additional context  |

### `feature_usage`

Tracks feature-specific interactions

| Column            | Type         | Description         |
| ----------------- | ------------ | ------------------- |
| `id`              | UUID         | Primary key         |
| `feature_name`    | VARCHAR(100) | Feature identifier  |
| `user_id`         | VARCHAR(32)  | Hashed user ID      |
| `organization_id` | VARCHAR(32)  | Hashed org ID       |
| `action`          | VARCHAR(100) | Action type         |
| `timestamp`       | TIMESTAMP    | Action time         |
| `deployment_type` | VARCHAR(50)  | cloud / self-hosted |
| `event_metadata`  | JSONB        | Additional context  |

## Configuration

### OpenTelemetry Collector

Configured via `apps/otel-collector/otel-collector-config.yaml`:

**Key Features:**

- OTLP gRPC and HTTP receivers with CORS support
- Batch processing for efficiency
- Memory limits to prevent OOM
- Automatic sensitive data filtering
- Resource metadata enrichment
- Event categorization for analytics

**Environment Variables:**

<CodeBlock filename="Terminal" language="bash">
{`TELEMETRY_PROCESSOR_ENDPOINT=telemetry-processor:4317
`}
</CodeBlock>

### Telemetry Processor

**Required Environment Variables:**

<CodeBlock filename="Terminal" language="bash">
{`# Analytics Database (separate from main database)
ANALYTICS_DB_USER=your-db-username
ANALYTICS_DB_PASS=your-secure-password
ANALYTICS_DB_HOST=postgres-analytics
ANALYTICS_DB_PORT=5432
ANALYTICS_DB_NAME=rhesis_analytics

# Or use connection string
ANALYTICS_DATABASE_URL=postgresql://user:pass@host:port/dbname

# Service Configuration
PORT=4317
LOG_LEVEL=INFO
`}
</CodeBlock>

## Local Development

### Starting the Services

<CodeBlock filename="Terminal" language="bash">
{`# Start analytics database and telemetry services
docker-compose up -d postgres-analytics otel-collector telemetry-processor

# View logs
docker-compose logs -f otel-collector telemetry-processor

# Check health
curl http://localhost:13133  # OTel Collector health
`}
</CodeBlock>

### Database Setup

The analytics database uses **Alembic** for migrations, which run automatically on container start.

<CodeBlock filename="Terminal" language="bash">
{`# Manual migration
cd apps/telemetry-processor
./migrate.sh

# Or use alembic directly
alembic upgrade head

# Check current schema version
alembic current
`}
</CodeBlock>

### Connect to Analytics Database

<CodeBlock filename="Terminal" language="bash">
{`# Via Docker Compose
docker-compose exec postgres-analytics psql -U your-db-username -d rhesis_analytics

# Check data
SELECT COUNT(*) FROM user_activity;
SELECT COUNT(*) FROM endpoint_usage;
SELECT COUNT(*) FROM feature_usage;
`}
</CodeBlock>

## Monitoring & Analytics

### Health Checks

<CodeBlock filename="Terminal" language="bash">
{`# OTel Collector health
curl http://localhost:13133

# Collector metrics
curl http://localhost:8888/metrics

# Debug traces
open http://localhost:55679/debug/tracez
`}
</CodeBlock>

### Sample Queries

**Recent Activity Summary:**

<CodeBlock filename="code.txt" language="sql">
{`SELECT
    event_type,
    COUNT(*) as count,
    MAX(timestamp) as latest
FROM user_activity
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY event_type;
`}
</CodeBlock>

**Top Endpoints by Usage:**

<CodeBlock filename="code.txt" language="sql">
{`SELECT
    endpoint,
    method,
    COUNT(*) as hits,
    AVG(duration_ms) as avg_duration_ms
FROM endpoint_usage
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY endpoint, method
ORDER BY hits DESC
LIMIT 10;
`}
</CodeBlock>

**Feature Adoption:**

<CodeBlock filename="code.txt" language="sql">
{`SELECT
    feature_name,
    action,
    COUNT(*) as count
FROM feature_usage
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY feature_name, action
ORDER BY count DESC;
`}
</CodeBlock>

**Overall Statistics:**

<CodeBlock filename="code.txt" language="sql">
{`SELECT
    'user_activity' as table_name,
    COUNT(*) as count
FROM user_activity
UNION ALL
SELECT 'endpoint_usage', COUNT(*) FROM endpoint_usage
UNION ALL
SELECT 'feature_usage', COUNT(*) FROM feature_usage;
`}
</CodeBlock>

## Troubleshooting

### No Data Appearing

1. **Check telemetry is enabled:**

<CodeBlock filename="Terminal" language="bash">
{`# In your .env file
TELEMETRY_ENABLED=true
DEPLOYMENT_TYPE=self-hosted  # or "cloud"`}
</CodeBlock>

2. **Verify OTel Collector is receiving data:**

<CodeBlock filename="Terminal" language="bash">
{`docker-compose logs otel-collector
# Look for "Traces received" messages`}
</CodeBlock>

3. **Check Telemetry Processor logs:**

<CodeBlock filename="Terminal" language="bash">
{`docker-compose logs telemetry-processor
# Look for "Processing span" messages`}
</CodeBlock>

4. **Test database connection:**
<CodeBlock filename="Terminal" language="bash">
{`docker-compose exec postgres-analytics psql -U user -d dbname -c "SELECT 1;"`}
</CodeBlock>

### Connection Issues

**OTel Collector can't reach Telemetry Processor:**

<CodeBlock filename="Terminal" language="bash">
{`# Check if processor is running
docker-compose ps telemetry-processor

# Check network connectivity
docker-compose exec otel-collector ping telemetry-processor
`}
</CodeBlock>

**Processor can't reach database:**

<CodeBlock filename="Terminal" language="bash">
{`# Verify database is running
docker-compose ps postgres-analytics

# Check logs
docker-compose logs postgres-analytics
`}
</CodeBlock>

### Performance Issues

1. **Check database indexes:**

<CodeBlock filename="code.txt" language="sql">
{`SELECT tablename, indexname
FROM pg_indexes
WHERE tablename IN ('user_activity', 'endpoint_usage', 'feature_usage');`}
</CodeBlock>

2. **Monitor active connections:**

<CodeBlock filename="code.txt" language="sql">
{`SELECT COUNT(*), state
FROM pg_stat_activity
WHERE datname = 'rhesis_analytics'
GROUP BY state;`}
</CodeBlock>

3. **Review collector memory usage:**
<CodeBlock filename="Terminal" language="bash">
{`docker stats otel-collector telemetry-processor`}
</CodeBlock>

## Deployment

### Google Cloud Run

Both services can be deployed to Cloud Run with Cloud SQL for the analytics database.

**Deployment workflows:**

- `.github/workflows/otel-collector.yml` - OTel Collector deployment
- `.github/workflows/telemetry-processor.yml` - Processor deployment

**Cloud SQL Connection:**

<CodeBlock filename="Terminal" language="bash">
{`ANALYTICS_DATABASE_URL=postgresql://user:pass@/dbname?host=/cloudsql/PROJECT:REGION:INSTANCE
`}
</CodeBlock>

### Kubernetes

Kubernetes manifests are available:

<CodeBlock filename="code.txt" language="text">
{`infrastructure/k8s/manifests/otel-collector-deployment.yaml
`}
</CodeBlock>

## Testing Locally

### Build and Run Collector

<CodeBlock filename="Terminal" language="bash">
{`cd apps/otel-collector
docker build -t rhesis-otel-collector .

docker run -p 4317:4317 -p 4318:4318 -p 13133:13133 \
  -e TELEMETRY_PROCESSOR_ENDPOINT=host.docker.internal:4317 \
  rhesis-otel-collector
`}
</CodeBlock>

### Build and Run Processor

<CodeBlock filename="Terminal" language="bash">
{`cd apps/telemetry-processor
docker build -t rhesis-telemetry-processor .

docker run -p 4317:4317 \
  -e ANALYTICS_DATABASE_URL=postgresql://user:pass@host/db \
  rhesis-telemetry-processor
`}
</CodeBlock>

## Development

### Project Structure

**OpenTelemetry Collector:**

<CodeBlock filename="code.txt" language="text">
{`apps/otel-collector/
├── otel-collector-config.yaml    # Collector configuration
├── Dockerfile                     # Container image
└── README.md                      # Component docs
`}
</CodeBlock>

**Telemetry Processor:**

<CodeBlock filename="code.txt" language="text">
{`apps/telemetry-processor/
├── src/processor/
│   ├── main.py                   # Entry point
│   ├── models/analytics.py       # Database models
│   ├── database/connection.py    # DB connection
│   ├── services/                 # Span processors
│   │   ├── user_activity.py
│   │   ├── endpoint_usage.py
│   │   ├── feature_usage.py
│   │   └── span_router.py
│   └── grpc/trace_service.py     # gRPC implementation
├── alembic/                       # Database migrations
├── Dockerfile                     # Container image
└── pyproject.toml                 # Dependencies
`}
</CodeBlock>

### Adding New Analytics

1. **Add model** to `models/analytics.py`:

<CodeBlock filename="example.py" language="python">
{`class NewFeatureAnalytics(AnalyticsBase):
__tablename__ = "new_feature_analytics"
custom_field = Column(String(100))`}
</CodeBlock>

2. **Create migration**:

<CodeBlock filename="Terminal" language="bash">
{`cd apps/telemetry-processor
alembic revision -m "add new feature analytics"`}
</CodeBlock>

3. **Add processor service** in `services/`:

<CodeBlock filename="example.py" language="python">
{`class NewFeatureProcessor(BaseSpanProcessor):
def process_span(self, span, resource, session):
# Processing logic`}
</CodeBlock>

4. **Update router** in `services/span_router.py`

5. **Run migration**:
<CodeBlock filename="Terminal" language="bash">
{`alembic upgrade head`}
</CodeBlock>

## Why Separate Analytics Database?

<Callout type="default">
  The telemetry processor uses a **dedicated PostgreSQL database** separate from
  the main application database.
</Callout>

**Benefits:**

✅ **Isolation** - Analytics doesn't affect operational database performance  
✅ **Security** - Different access controls and permissions  
✅ **Scalability** - Scale independently based on analytics volume  
✅ **Backup** - Different retention policies for analytics vs operational data  
✅ **Privacy** - Easier to manage compliance and data retention  
✅ **Performance** - Optimized for analytics workloads

## Learn More

For implementation details, see:

- `apps/otel-collector/otel-collector-config.yaml` - Collector configuration
- `apps/telemetry-processor/src/` - Processor source code
- `apps/telemetry-processor/alembic/versions/` - Database migrations
